{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "machine-learning-project-sales-forecastin.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/B1u3B01t/Sales-Forecasting/blob/main/SalesForecasting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "scrolled": false,
        "id": "6RfWJBOjdIe9"
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J3DJMIedIe_"
      },
      "source": [
        "Project Phases:\n",
        "* Libraries and Data Loading\n",
        "* Exploratory Analysis and Data Cleaning\n",
        "* Machine Learning\n",
        "* Christmas Adjustment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtJYohspdIfA"
      },
      "source": [
        "# 1. Libraries and Data Loading \n",
        "\n",
        "We will be loading some libraries to make processing easier and some to view the data\n",
        "libraries used :\n",
        "* Pandas\n",
        "* Numpy\n",
        "* MatPlotLib\n",
        "* Seaborn\n",
        "* SciPy\n",
        "* SkLearn\n",
        "* PandaSQL\n",
        "* Warning (ignore any warning we might come across)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "mr8z2c8OdIfA"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.special import boxcox1p\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# \n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") # ignoring annoying warnings\n",
        "\n",
        "from pandasql import sqldf\n",
        "pysqldf = lambda q: sqldf(q, globals())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "l0_ZRLbUdIfB"
      },
      "source": [
        "features = pd.read_csv('../input/dataset/features.csv')\n",
        "train = pd.read_csv('../input/dataset/train.csv')\n",
        "stores = pd.read_csv('../input/dataset/stores.csv')\n",
        "test = pd.read_csv('../input/dataset/test.csv')\n",
        "sample_submission = pd.read_csv('../input/dataset/sampleSubmission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIgs-poUdIfB"
      },
      "source": [
        "# 2. Exploratory Analysis and Data Cleaning\n",
        "* Merging \"Features\" and \"Stores\"\n",
        "* Adding Easter to isHoliday\n",
        "* Dropping Features with low correlation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "JVHSdVV_dIfC"
      },
      "source": [
        "feat_sto = features.merge(stores, how='inner', on='Store')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "SbrMo_yHdIfC"
      },
      "source": [
        "feat_sto.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "QkkZbmfydIfC"
      },
      "source": [
        "pd.DataFrame(feat_sto.dtypes, columns=['Type'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "AKXm3BePdIfD"
      },
      "source": [
        "pd.DataFrame({'Type_Train': train.dtypes, 'Type_Test': test.dtypes})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "oUpBqXWwdIfD"
      },
      "source": [
        "feat_sto.Date = pd.to_datetime(feat_sto.Date)\n",
        "train.Date = pd.to_datetime(train.Date)\n",
        "test.Date = pd.to_datetime(test.Date)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "b5Lmea47dIfD"
      },
      "source": [
        "feat_sto['Week'] = feat_sto.Date.dt.week \n",
        "feat_sto['Year'] = feat_sto.Date.dt.year"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "XsMvXawydIfE"
      },
      "source": [
        "train_detail = train.merge(feat_sto, \n",
        "                           how='inner',\n",
        "                           on=['Store','Date','IsHoliday']).sort_values(by=['Store','Dept','Date']).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "suKShjLndIfE"
      },
      "source": [
        "train_detail.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "rbOqp9OudIfE"
      },
      "source": [
        "test_detail = test.merge(feat_sto, \n",
        "                           how='inner',\n",
        "                           on=['Store','Date','IsHoliday']).sort_values(by=['Store',\n",
        "                                                                            'Dept',\n",
        "                                                                            'Date']).reset_index(drop=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "LsC3Jk2ldIfF"
      },
      "source": [
        "del features, train, stores, test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "QuWdKbF6dIfF"
      },
      "source": [
        "null_columns = (train_detail.isnull().sum(axis = 0)/len(train_detail)).sort_values(ascending=False).index\n",
        "null_data = pd.concat([\n",
        "    train_detail.isnull().sum(axis = 0),\n",
        "    (train_detail.isnull().sum(axis = 0)/len(train_detail)).sort_values(ascending=False),\n",
        "    train_detail.loc[:, train_detail.columns.isin(list(null_columns))].dtypes], axis=1)\n",
        "null_data = null_data.rename(columns={0: '# null', \n",
        "                                      1: '% null', \n",
        "                                      2: 'type'}).sort_values(ascending=False, by = '% null')\n",
        "null_data = null_data[null_data[\"# null\"]!=0]\n",
        "null_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxX6A3PndIfF"
      },
      "source": [
        "# Holidays Analysis\n",
        "\n",
        "If, for a certain Week, there are more pre-holiday days in one Year than another, then it is very possible that the Year with more pre-holiday days will have greater Sales for the same Week. So, the model will not take this consideration and we might need to adjust the predicted values at the end.\n",
        "\n",
        "Another thing to take into account is that Holiday Weeks but with few or no pre-holiday days might have lower Sales than the Week before.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "Cul0BHYKdIfG"
      },
      "source": [
        "pysqldf(\"\"\"\n",
        "SELECT\n",
        "    T.*,\n",
        "    case\n",
        "        when ROW_NUMBER() OVER(partition by Year order by week) = 1 then 'Super Bowl'\n",
        "        when ROW_NUMBER() OVER(partition by Year order by week) = 2 then 'Labor Day'\n",
        "        when ROW_NUMBER() OVER(partition by Year order by week) = 3 then 'Thanksgiving'\n",
        "        when ROW_NUMBER() OVER(partition by Year order by week) = 4 then 'Christmas'\n",
        "    end as Holyday,\n",
        "    case\n",
        "        when ROW_NUMBER() OVER(partition by Year order by week) = 1 then 'Sunday'\n",
        "        when ROW_NUMBER() OVER(partition by Year order by week) = 2 then 'Monday'\n",
        "        when ROW_NUMBER() OVER(partition by Year order by week) = 3 then 'Thursday'\n",
        "        when ROW_NUMBER() OVER(partition by Year order by week) = 4 and Year = 2010 then 'Saturday'\n",
        "        when ROW_NUMBER() OVER(partition by Year order by week) = 4 and Year = 2011 then 'Sunday'\n",
        "        when ROW_NUMBER() OVER(partition by Year order by week) = 4 and Year = 2012 then 'Tuesday'\n",
        "    end as Day\n",
        "    from(\n",
        "        SELECT DISTINCT\n",
        "            Year,\n",
        "            Week,\n",
        "            case \n",
        "                when Date <= '2012-11-01' then 'Train Data' else 'Test Data' \n",
        "            end as Data_type\n",
        "        FROM feat_sto\n",
        "        WHERE IsHoliday = True) as T\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP_DqysOdIfG"
      },
      "source": [
        "## **Observation 1**\n",
        "\n",
        "* All Holidays fall on the same week\n",
        "* Christmas has 0 pre-holiday days in 2010, 1 in 2011 and 3 in 2012. The model will not consider more Sales in 2012 for Test Data, so we are going to adjust it at the end, with a formula and an explanation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YxjdeXGdIfH"
      },
      "source": [
        "# Average Weekly Sales - Per Year"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "AKKf23zodIfI"
      },
      "source": [
        "weekly_sales_2010 = train_detail[train_detail.Year==2010]['Weekly_Sales'].groupby(train_detail['Week']).mean()\n",
        "weekly_sales_2011 = train_detail[train_detail.Year==2011]['Weekly_Sales'].groupby(train_detail['Week']).mean()\n",
        "weekly_sales_2012 = train_detail[train_detail.Year==2012]['Weekly_Sales'].groupby(train_detail['Week']).mean()\n",
        "plt.figure(figsize=(20,10))\n",
        "sns.lineplot(weekly_sales_2010.index, weekly_sales_2010.values)\n",
        "sns.lineplot(weekly_sales_2011.index, weekly_sales_2011.values)\n",
        "sns.lineplot(weekly_sales_2012.index, weekly_sales_2012.values)\n",
        "plt.grid()\n",
        "plt.xticks(np.arange(1, 53, step=1))\n",
        "plt.legend(['2010', '2011', '2012'], loc='best', fontsize=16)\n",
        "plt.title('Average Weekly Sales - Per Year', fontsize=18)\n",
        "plt.ylabel('Sales', fontsize=16)\n",
        "plt.xlabel('Week', fontsize=16)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM4DaXg7dIfI"
      },
      "source": [
        "## Observation 2\n",
        "As we can see, there is one important Holiday not included in 'IsHoliday'. It's the Easter Day. It is always in a Sunday, but can fall on different weeks.\n",
        "\n",
        "In 2010 is in Week 13\n",
        "In 2011, Week 16\n",
        "Week 14 in 2012\n",
        "and, finally, Week 13 in 2013 for Test set\n",
        "So, we can change to 'True' these Weeks in each Year."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnxOVJZ5dIfI"
      },
      "source": [
        "### **Adding Easter**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "x3SxBAamdIfJ"
      },
      "source": [
        "train_detail.loc[(train_detail.Year==2010) & (train_detail.Week==13), 'IsHoliday'] = True\n",
        "train_detail.loc[(train_detail.Year==2011) & (train_detail.Week==16), 'IsHoliday'] = True\n",
        "train_detail.loc[(train_detail.Year==2012) & (train_detail.Week==14), 'IsHoliday'] = True\n",
        "test_detail.loc[(test_detail.Year==2013) & (test_detail.Week==13), 'IsHoliday'] = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhcZhbQodIfJ"
      },
      "source": [
        "# Weekly Sales - Mean and Median"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "LdvppwT-dIfJ"
      },
      "source": [
        "weekly_sales_mean = train_detail['Weekly_Sales'].groupby(train_detail['Date']).mean()\n",
        "weekly_sales_median = train_detail['Weekly_Sales'].groupby(train_detail['Date']).median()\n",
        "\n",
        "plt.figure(figsize=(20,8))\n",
        "sns.lineplot(weekly_sales_mean.index, weekly_sales_mean.values)\n",
        "sns.lineplot(weekly_sales_median.index, weekly_sales_median.values)\n",
        "\n",
        "plt.grid()\n",
        "plt.legend(['Mean', 'Median'], loc='best', fontsize=16)\n",
        "plt.title('Weekly Sales - Mean and Median', fontsize=18)\n",
        "plt.ylabel('Sales', fontsize=16)\n",
        "plt.xlabel('Date', fontsize=16)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx1QWdMpdIfJ"
      },
      "source": [
        "# Average Sales per Store and Department"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "sZuPyhhMdIfK"
      },
      "source": [
        "weekly_sales = train_detail['Weekly_Sales'].groupby(train_detail['Store']).mean()\n",
        "plt.figure(figsize=(20,10))\n",
        "sns.barplot(weekly_sales.index, weekly_sales.values, palette='dark')\n",
        "plt.grid()\n",
        "plt.title('Average Sales - per Store', fontsize=18)\n",
        "plt.ylabel('Sales', fontsize=16)\n",
        "plt.xlabel('Store', fontsize=16)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Btay5ViBdIfK"
      },
      "source": [
        "# Average Sales - Department Wise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "hxVin3hAdIfK"
      },
      "source": [
        "weekly_sales = train_detail['Weekly_Sales'].groupby(train_detail['Dept']).mean()\n",
        "plt.figure(figsize=(20,10))\n",
        "sns.barplot(weekly_sales.index, weekly_sales.values, palette='dark')\n",
        "plt.grid()\n",
        "plt.title('Average Sales - per Dept', fontsize=18)\n",
        "plt.ylabel('Sales', fontsize=16)\n",
        "plt.xlabel('Dept', fontsize=16)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE3Du3tjdIfK"
      },
      "source": [
        "## Obeservation 3\n",
        "There are Sales difference between the Store and Departments. Also some Depts are not in the list, like number '15', for example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Alf7wF9MdIfL"
      },
      "source": [
        "# Correlation Matrix - Pearson Correlation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdzXyDsQdIfL"
      },
      "source": [
        "Correlation Metrics:\n",
        "\n",
        "* 0: no correlation at all\n",
        "* 0-0.3: weak correlation\n",
        "* 0.3-0.7: moderate correlaton\n",
        "* 0.7-1: strong correlation\n",
        "\n",
        "**Positive Correlation indicates that when one variable increase, the other also does. Negative is the opposite.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "wO8lsHlzdIfL"
      },
      "source": [
        "sns.set(style=\"white\")\n",
        "\n",
        "corr = train_detail.corr()\n",
        "\n",
        "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
        "\n",
        "f, ax = plt.subplots(figsize=(20, 20))\n",
        "\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "plt.title('Correlation Matrix', fontsize=18)\n",
        "\n",
        "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssuEBH6DdIfM"
      },
      "source": [
        "### Observation 4\n",
        "'**MarkDown**' 1 to 5 are not strong correlated to '**Weekly_Sales**' and they have a lot of null values, then we can drop them.\n",
        "\n",
        "Also, '**Fuel_Price**' is strong correlated to '**Year**'. One of them must be dropped else they would carry similar information to the model. 'Year' will not be dropped, because it differentiate same Weeks for 'Store'+'Dept'.\n",
        "\n",
        "Other variables that have weak correlation with '**Weekly_Sales**' can be analyzed to see if they are useful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsxyaYjFdIfM"
      },
      "source": [
        "**Droping Markdown values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "Q6IlK_hvdIfM"
      },
      "source": [
        "train_detail = train_detail.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'])\n",
        "test_detail = test_detail.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqOmyrN6dIfN"
      },
      "source": [
        "# Analysing Variables\n",
        "1. BoxPlot and StripPlot for Discrete Variables\n",
        "2. BoxCox for Continous Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "ekIDo7MCdIfN"
      },
      "source": [
        "def boxplot(feature):\n",
        "    fig = plt.figure(figsize=(20,8))\n",
        "    gs = GridSpec(1,2)\n",
        "    sns.boxplot(y=train_detail.Weekly_Sales, x=train_detail[feature], ax=fig.add_subplot(gs[0,0]))\n",
        "    plt.ylabel('Sales', fontsize=16)\n",
        "    plt.xlabel(feature, fontsize=16)\n",
        "    sns.stripplot(y=train_detail.Weekly_Sales, x=train_detail[feature], ax=fig.add_subplot(gs[0,1]))\n",
        "    plt.ylabel('Sales', fontsize=16)\n",
        "    plt.xlabel(feature, fontsize=16)\n",
        "    fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "CGxhlag0dIfN"
      },
      "source": [
        "def boxcox(feature):\n",
        "    \n",
        "    fig = plt.figure(figsize=(18,15))\n",
        "    gs = GridSpec(2,2)\n",
        "    \n",
        "    j = sns.scatterplot(y=train_detail['Weekly_Sales'], \n",
        "                        x=boxcox1p(train_detail[feature], 0.15), ax=fig.add_subplot(gs[0,1]), palette = 'blue')\n",
        "\n",
        "    plt.title('BoxCox 0.15\\n' + 'Corr: ' + str(np.round(train_detail['Weekly_Sales'].corr(boxcox1p(train_detail[feature], 0.15)),2)) +\n",
        "              ', Skew: ' + str(np.round(stats.skew(boxcox1p(train_detail[feature], 0.15), nan_policy='omit'),2)))\n",
        "    \n",
        "    j = sns.scatterplot(y=train_detail['Weekly_Sales'], \n",
        "                        x=boxcox1p(train_detail[feature], 0.25), ax=fig.add_subplot(gs[1,0]), palette = 'blue')\n",
        "\n",
        "    plt.title('BoxCox 0.25\\n' + 'Corr: ' + str(np.round(train_detail['Weekly_Sales'].corr(boxcox1p(train_detail[feature], 0.25)),2)) +\n",
        "              ', Skew: ' + str(np.round(stats.skew(boxcox1p(train_detail[feature], 0.25), nan_policy='omit'),2)))\n",
        "    \n",
        "    j = sns.distplot(train_detail[feature], ax=fig.add_subplot(gs[1,1]), color = 'green')\n",
        "\n",
        "    plt.title('Distribution\\n')\n",
        "    \n",
        "    j = sns.scatterplot(y=train_detail['Weekly_Sales'], \n",
        "                        x=train_detail[feature], ax=fig.add_subplot(gs[0,0]), color = 'red')\n",
        "\n",
        "    plt.title('Linear\\n' + 'Corr: ' + str(np.round(train_detail['Weekly_Sales'].corr(train_detail[feature]),2)) + ', Skew: ' + \n",
        "               str(np.round(stats.skew(train_detail[feature], nan_policy='omit'),2)))\n",
        "    \n",
        "    fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTntDNwOdIfN"
      },
      "source": [
        "# Weekly Sales vs IsHoliday"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "93o7VXUIdIfN"
      },
      "source": [
        "boxplot('IsHoliday')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "LoSRb3etdIfO"
      },
      "source": [
        "boxplot('Type')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEPV24EcdIfO"
      },
      "source": [
        "We don't know what 'Type' is, but we can assume that A > B > C in terms of Sales Median. So, let's treat it as an ordinal variable and replace its values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "QOijshkSdIfO"
      },
      "source": [
        "train_detail.Type = train_detail.Type.apply(lambda x: 3 if x == 'A' else(2 if x == 'B' else 1))\n",
        "test_detail.Type = test_detail.Type.apply(lambda x: 3 if x == 'A' else(2 if x == 'B' else 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CCDxb65dIfO"
      },
      "source": [
        "# Weekly Sales vs Temperature (Continuous)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "n34FOnmzdIfP"
      },
      "source": [
        "boxcox('Temperature')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-eJFPW6dIfP"
      },
      "source": [
        "The correlations doesn't seem to change at any skewness toh we will drop this feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qswwT8cBdIfP"
      },
      "source": [
        "### Droping Temperature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "EYa22V1AdIfP"
      },
      "source": [
        "train_detail = train_detail.drop(columns=['Temperature'])\n",
        "test_detail = test_detail.drop(columns=['Temperature'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C7nj47cdIfP"
      },
      "source": [
        "# Weekly Sales vs Unemployement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "id": "og6ihjXmdIfP"
      },
      "source": [
        "boxcox('Unemployment')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVl6hWL4dIfQ"
      },
      "source": [
        "Same for **Unemployment** rate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Y1kkPOVwdIfQ"
      },
      "source": [
        "train_detail = train_detail.drop(columns=['Unemployment'])\n",
        "test_detail = test_detail.drop(columns=['Unemployment'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Lbdj6PtdIfQ"
      },
      "source": [
        "# Weekly Sales vs CPI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "j5HKd4QidIfQ"
      },
      "source": [
        "boxcox('CPI')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH7tn7CMdIfQ"
      },
      "source": [
        "Same for **CPI**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yWzBkDuwdIfR"
      },
      "source": [
        "train_detail = train_detail.drop(columns=['CPI'])\n",
        "test_detail = test_detail.drop(columns=['CPI'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inesSnkCdIfR"
      },
      "source": [
        "# Weekly Sales vs Size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "dEFBmB3kdIfR"
      },
      "source": [
        "boxcox('Size')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73bzLzYbdIfR"
      },
      "source": [
        "We will continue with this variable, since it has moderate correlation with **WeeklySales**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQGhL8CrdIfR"
      },
      "source": [
        "# Weighted Mean Average Error Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1UN-HTshdIfS"
      },
      "source": [
        "def WMAE(dataset, real, predicted):\n",
        "    weights = dataset.IsHoliday.apply(lambda x: 5 if x else 1)\n",
        "    return np.round(np.sum(weights*abs(real-predicted))/(np.sum(weights)), 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "S7gZwVANdIfS"
      },
      "source": [
        "train_detail"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WOAah0MLdIfS"
      },
      "source": [
        "test_detail"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbNAXkoCdIfS"
      },
      "source": [
        "# Algorithms (Without thinking about efficency)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "AenfMEHOdIfS"
      },
      "source": [
        "def knn():\n",
        "    knn = KNeighborsRegressor(n_neighbors=10)\n",
        "    return knn\n",
        "\n",
        "def extraTreesRegressor():\n",
        "    clf = ExtraTreesRegressor(n_estimators=100,max_features='auto', verbose=1, n_jobs=1)\n",
        "    return clf\n",
        "    \n",
        "def randomForestRegressor():\n",
        "    clf = RandomForestRegressor(n_estimators=100,max_features='log2', verbose=1)\n",
        "    return clf\n",
        "\n",
        "def svm():\n",
        "    clf = SVR(kernel='rbf', gamma='auto')\n",
        "    return clf\n",
        "\n",
        "def nn():\n",
        "    clf = MLPRegressor(hidden_layer_sizes=(10,),  activation='relu', verbose=3)\n",
        "    return clf\n",
        "\n",
        "def predict_(m, test_x):\n",
        "    return pd.Series(m.predict(test_x))\n",
        "\n",
        "def model_():\n",
        "#     return knn()\n",
        "#     return extraTreesRegressor()\n",
        "    return svm()\n",
        "#     return nn()\n",
        "#     return randomForestRegressor()    \n",
        "\n",
        "def train_(train_x, train_y):\n",
        "    m = model_()\n",
        "    m.fit(train_x, train_y)\n",
        "    return m\n",
        "\n",
        "def train_and_predict(train_x, train_y, test_x):\n",
        "    m = train_(train_x, train_y)\n",
        "    return predict_(m, test_x), m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VH8E7vDdIfT"
      },
      "source": [
        "# Random Forest (Split into 3 parts for optimizing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "lsQrNYpCdIfT"
      },
      "source": [
        "def random_forest(n_estimators, max_depth):\n",
        "    result = []\n",
        "    for estimator in n_estimators:\n",
        "        for depth in max_depth:\n",
        "            wmaes_cv = []\n",
        "            for i in range(1,5):\n",
        "                print('k:', i, ', n_estimators:', estimator, ', max_depth:', depth)\n",
        "                x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n",
        "                RF = RandomForestRegressor(n_estimators=estimator, max_depth=depth)\n",
        "                RF.fit(x_train, y_train)\n",
        "                predicted = RF.predict(x_test)\n",
        "                wmaes_cv.append(WMAE(x_test, y_test, predicted))\n",
        "            print('WMAE:', np.mean(wmaes_cv))\n",
        "            result.append({'Max_Depth': depth, 'Estimators': estimator, 'WMAE': np.mean(wmaes_cv)})\n",
        "    return pd.DataFrame(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "jG8Nbr9YdIfT"
      },
      "source": [
        "def random_forest_II(n_estimators, max_depth, max_features):\n",
        "    result = []\n",
        "    for feature in max_features:\n",
        "        wmaes_cv = []\n",
        "        for i in range(1,5):\n",
        "            print('k:', i, ', max_features:', feature)\n",
        "            x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n",
        "            RF = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=feature)\n",
        "            RF.fit(x_train, y_train)\n",
        "            predicted = RF.predict(x_test)\n",
        "            wmaes_cv.append(WMAE(x_test, y_test, predicted))\n",
        "        print('WMAE:', np.mean(wmaes_cv))\n",
        "        result.append({'Max_Feature': feature, 'WMAE': np.mean(wmaes_cv)})\n",
        "    return pd.DataFrame(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2uizU2CmdIfT"
      },
      "source": [
        "def random_forest_III(n_estimators, max_depth, max_features, min_samples_split, min_samples_leaf):\n",
        "    result = []\n",
        "    for split in min_samples_split:\n",
        "        for leaf in min_samples_leaf:\n",
        "            wmaes_cv = []\n",
        "            for i in range(1,5):\n",
        "                print('k:', i, ', min_samples_split:', split, ', min_samples_leaf:', leaf)\n",
        "                x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n",
        "                RF = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features, \n",
        "                                           min_samples_leaf=leaf, min_samples_split=split)\n",
        "                RF.fit(x_train, y_train)\n",
        "                predicted = RF.predict(x_test)\n",
        "                wmaes_cv.append(WMAE(x_test, y_test, predicted))\n",
        "            print('WMAE:', np.mean(wmaes_cv))\n",
        "            result.append({'Min_Samples_Leaf': leaf, 'Min_Samples_Split': split, 'WMAE': np.mean(wmaes_cv)})\n",
        "    return pd.DataFrame(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hbwIsRBdIfT"
      },
      "source": [
        "# Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oUosVlNydIfU"
      },
      "source": [
        "X_train = train_detail[['Store','Dept','IsHoliday','Size','Week','Type','Year']]\n",
        "Y_train = train_detail['Weekly_Sales']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h_F7nMzdIfU"
      },
      "source": [
        "# K-FOLD CROSS VALIDATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kJiXJmQAdIfU"
      },
      "source": [
        "kf = KFold(n_splits=5)\n",
        "splited = []\n",
        "# dataset2 = dataset.copy()\n",
        "for name, group in train_detail.groupby([\"Store\", \"Dept\"]):\n",
        "    group = group.reset_index(drop=True)\n",
        "    trains_x = []\n",
        "    trains_y = []\n",
        "    tests_x = []\n",
        "    tests_y = []\n",
        "    if group.shape[0] <= 5:\n",
        "        f = np.array(range(5))\n",
        "        np.random.shuffle(f)\n",
        "        group['fold'] = f[:group.shape[0]]\n",
        "        continue\n",
        "    fold = 0\n",
        "    for train_index, test_index in kf.split(group):\n",
        "        group.loc[test_index, 'fold'] = fold\n",
        "        fold += 1\n",
        "    splited.append(group)\n",
        "\n",
        "splited = pd.concat(splited).reset_index(drop=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YVA-uX5WdIfU"
      },
      "source": [
        "splited"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BzrrKYiZdIfU"
      },
      "source": [
        "best_model = None\n",
        "error_cv = 0\n",
        "best_error = np.iinfo(np.int32).max\n",
        "for fold in range(5):\n",
        "    train_detail = splited.loc[splited['fold'] != fold]\n",
        "    test_detail = splited.loc[splited['fold'] == fold]\n",
        "    train_y = train_detail['Weekly_Sales']\n",
        "    train_x = train_detail[['Store','Dept','IsHoliday','Size','Week','Type','Year']]\n",
        "    test_y = test_detail['Weekly_Sales']\n",
        "    test_x = test_detail[['Store', 'Dept', 'IsHoliday', 'Size', 'Week', 'Type', 'Year']]\n",
        "    print(train_detail.shape, test_detail.shape)\n",
        "    predicted, model = train_and_predict(train_x, train_y, test_x)\n",
        "#     weights = test_x['isHoliday'].replace(True, 5).replace(False, 1)\n",
        "#     error = calculate_error(test_y, predicted, weights)\n",
        "    error = WMAE(test_x, test_y, predicted)\n",
        "    print(error)\n",
        "    error_cv += error\n",
        "    print(fold, error)\n",
        "    if error < best_error:\n",
        "        print('Find best model')\n",
        "        best_error = error\n",
        "        best_model = model\n",
        "error_cv /= 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "o9ZbTRR0dIfV"
      },
      "source": [
        "error_cv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JIZhPib_dIfV"
      },
      "source": [
        "best_error\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tSpxkCvdIfV"
      },
      "source": [
        "Tuning 'n_estimators' and 'max_depth'.\n",
        "\n",
        "Here, it is possible to test a lot of values. These are the final ones, after a bit of testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpIIiUxbdIfV"
      },
      "source": [
        "## Tuning n_estimators and max_depth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NYEpNQWUdIfV"
      },
      "source": [
        "n_estimators = [56, 58, 60]\n",
        "max_depth = [25, 27, 30]\n",
        "\n",
        "random_forest(n_estimators, max_depth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxp9l9kgdIfV"
      },
      "source": [
        "The result based on **WAME**\n",
        "* Max_Depth - 25\n",
        "* Estimators - 56"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ8pPE2FdIfW"
      },
      "source": [
        "# Tuning max_features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "EPIrdlPmdIfW"
      },
      "source": [
        "max_features = [2, 3, 4, 5, 6, 7]\n",
        "\n",
        "random_forest_II(n_estimators=56, max_depth=25, max_features=max_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnZafnsrdIfW"
      },
      "source": [
        "Max_Feature - 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXl5Em2mdIfW"
      },
      "source": [
        "## Tuning min_samples_spilt and min_samples_leaf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "noTWl8e7dIfW"
      },
      "source": [
        "min_samples_split = [2, 3, 4]\n",
        "min_samples_leaf = [1, 2, 3]\n",
        "\n",
        "random_forest_III(n_estimators=56, max_depth=25, max_features=7, \n",
        "                  min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNGedewNdIfX"
      },
      "source": [
        "Min_Samples_Leaf - 1 \\\n",
        "Min_Samples_Split - 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcdq06bbdIfX"
      },
      "source": [
        "# Final Fitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JsSO5KbgdIfX"
      },
      "source": [
        "RF = RandomForestRegressor(n_estimators=56, max_depth=25, max_features=7, min_samples_split=2, min_samples_leaf=1)\n",
        "RF.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAM3KJYgdIfX"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tJ18ii_cdIfX"
      },
      "source": [
        "X_test = test_detail[['Store', 'Dept', 'IsHoliday', 'Size', 'Week', 'Type', 'Year']]\n",
        "predict = RF.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "eJskcvWOdIfX"
      },
      "source": [
        "sample_submission['Weekly_Sales'] = predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "C1867r6DdIfY"
      },
      "source": [
        "sample_submission"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6x9QcSchdIfY"
      },
      "source": [
        "sample_submission.to_csv('Submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4gZ0VpJgdIfY"
      },
      "source": [
        "f = open(\"Submission.csv\", \"x\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5GNiKnoodIfY"
      },
      "source": [
        "from IPython.display import HTML\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import base64\n",
        "\n",
        "# function that takes in a dataframe and creates a text link to  \n",
        "# download it (will only work for files < 2MB or so)\n",
        "def create_download_link(df, title = \"Download CSV file\", filename = \"submission.csv\"):  \n",
        "    csv = df.to_csv(index=False)\n",
        "    b64 = base64.b64encode(csv.encode())\n",
        "    payload = b64.decode()\n",
        "    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n",
        "    html = html.format(payload=payload,title=title,filename=filename)\n",
        "    return HTML(html)\n",
        "\n",
        "# create a link to download the dataframe\n",
        "create_download_link(sample_submission)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-9G82j0KdIfY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}